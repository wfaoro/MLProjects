{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "William Faoro PS7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhcnqfawZhmx"
      },
      "source": [
        "# Problem Set 7: Backpropagation\n",
        "# CMSC 422, Fall 2021\n",
        "# Due Nov 18 at 11:59pm\n",
        "\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/max/1914/1*F9capAHwl_rz2-Q8z511WQ.jpeg\" alt=\"meme\" width=\"500px\"/>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ooh3KTatYqgl"
      },
      "source": [
        "# Instructions\n",
        "In this problem set you will implement backpropagation for a set of different neural network architectures.\n",
        "There is some code provided for you here, and you will write your implementations in the places marked with __```#TODO: Your Code Here```__. You may add helper functions if you feel you need to.\n",
        "\n",
        "__Analysis Questions:__ In addition to Python programming, each problem will contain some analysis questions (under __Analysis__). These are meant to ensure you understand your results, and will be manually graded on Gradescope.\n",
        "\n",
        "__Submission:__ download this notebook as a `.ipynb` file and submit it to Gradescope. This assignment will be partially autograded so follow instructions closely.  \n",
        " \n",
        "- Make sure your plots are visible when downloading the notebook, otherwise they won't appear on Gradescope. \n",
        "- Make sure your code cells are not throwing exceptions.\n",
        "- Please do not import any packages other than what has already been imported here. You may be penalized for doing so.\n",
        "- Lastly, the autograder times out after 40 minutes, so make sure your implementation is relativly efficient (e.g. by using numpy for matrix operations). Our implementation took a little over 10 minutes to test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOtWraGuaWzc"
      },
      "source": [
        "# Problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noBuJ-fIZJ0m"
      },
      "source": [
        "## Problem 1 (25 Points)\n",
        "We'll begin with the simplest possible network (a single layer perceptron). It has a single input feature that we call $x$. This is the activation of the single node of the input layer. This is connected to a single output node, which has a weight, $w$. We also have a bias term, so the activation of the output unit is $a = wx + b$. This network will be used to solve a linear regression problem. So, if we are given an input pair of $(x,y)$, we want to minimize the loss: \n",
        "\n",
        "$$L(x,y) = \\frac{1}{1+e^{-(a-y)^2}} - \\frac{1}{2}$$\n",
        "\n",
        "To do this, you will need to randomly initialize the weight and bias and then perform gradient descent.\n",
        "<br>\n",
        "<br>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/thumbnail?id=1Qz8jJaPXbVzoL44Nd4nbQgFHOA8G_XyI&sz=w1000\" alt=\"net1\" width=\"150px\"/>\n",
        "<br>\n",
        "<i>Figure 1: Network for Problem 1</i>\n",
        "</center>\n",
        "<br>\n",
        "<br>\n",
        "The gradient of the loss is computed using a training set containing pairs, $(x_1, y_1), (x_2, y_2), ... (x_n, y_n)$. We have:\n",
        "\n",
        "$$\\nabla L = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{\\partial L}{\\partial w}(x_i, y_i), \\frac{\\partial L}{\\partial b}(x_i, y_i)  \\right)$$\n",
        "\n",
        "If we denote $\\theta = (w,b)$ as a vector containing all the parameters of the network, we perform gradient\n",
        "descent with the update:\n",
        "\n",
        "$$\\theta^k = \\theta^{k-1} - \\eta \\nabla L$$\n",
        "\n",
        "Here $\\eta$ is the learning rate, and $\\theta^k$ denotes a vector of $(w,b)$ after the $k$'th iteration of gradient descent. Do not mistake $\\eta$ (the learning rate) for $n$ (the number of data points).\n",
        "We provide you with a routine to generate training data. This has the form: \n",
        "\n",
        "```simplest_training_data(n)```  \n",
        "  \n",
        "This just generates $n$ random training points on the line $y = 3x + 2$, with a little Gaussian noise added to the points.  \n",
        "You need to write a routine with the form: \n",
        "\n",
        "```simplest_training(n, k, eta)```\n",
        "\n",
        "Here $n$ indicates the number of points in the training set (you can call `simplest_training_data` to get the training data), $k$ indicates the total number of iterations that you will use in training, and $\\eta$ is the learning rate.  To initialize the weights in your network, we suggest that you initialize $w$ with a Gaussian random variable with mean 0 and variance of 1, and that you initialize $b = 0$.  \n",
        "You also need to write a routine of the form: \n",
        "\n",
        "```simplest_testing(theta, x)```\n",
        "\n",
        "This routine applies the network, using the parameters in theta, to the input values in the vector $x$, and returns a vector of results in $y$.\n",
        "After training, the network should learn $w$ and $b$ values that are similar to those used to train the network.  So you can test your network by looking at the learned $w$ and $b$ values.  Or you can use the testing algorithm to see if the network computes appropriate $y$ values.  In testing, you may find that if you use too big a value for $\\eta$ the network will not converge to anything meaningful.  If you use a value of $k$ that is too small, it won't have time to converge to a good solution.\n",
        "We run our algorithm with $n = 30, k = 10000, \\eta = .02$.   When we test using $x = (0, 1, ..., 9)$ we get the result:\n",
        "  \n",
        "```\n",
        "1.99107688  4.91908171  7.84708654 10.77509137 13.7030962  16.63110103 19.55910586 22.48711069 25.41511552 28.34312035\n",
        "```\n",
        "\n",
        "These points fit the line $y = 3x + 2$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZsEHnFFs01Z"
      },
      "source": [
        "import numpy as np\n",
        "import math as m\n",
        "import sys\n",
        "\n",
        "###Problem 1\n",
        "###Provided function to create training data\n",
        "def simplest_training_data(n): # Do NOT edit this function: simplest_training_data(n)!\n",
        "    m = 3\n",
        "    b = 2\n",
        "    x = np.random.uniform(0,1,n)\n",
        "    y = m*x+b+0.3*np.random.normal(0,1,n)\n",
        "    return (x,y)\n",
        "    \n",
        "def find_dev(x,y,w,b):\n",
        "  z = ((w*x) + b - y)**2\n",
        "  dl_dw = 2 * np.exp(-z)*(1+np.exp(-z))**(-2)*x*(w*x + b - y)\n",
        "  dl_db = 2 * np.exp(-z)*(1+np.exp(-z))**(-2)*(w*x + b - y)*1\n",
        "  return dl_dw,dl_db\n",
        "\n",
        "def simplest_training(n, k, eta):\n",
        "  np.random.seed(0)\n",
        "  # TODO: implement this method.\n",
        "  # Perform gradient descent k times with learning rate eta\n",
        "  # and n training data points.\n",
        "  # theta contains the learned parameters after training.\n",
        "  trainX, trainY = simplest_training_data(n)\n",
        "  w = np.random.normal(0,1)\n",
        "  b = 0\n",
        "  for i in range(k):\n",
        "    loss_grad = [0,0]\n",
        "    dl_dw_epoch = 0\n",
        "    dl_db_epoch = 0\n",
        "    for j in range(n):\n",
        "      dl_dw,dl_db = find_dev(trainX[j],trainY[j],w,b)\n",
        "      dl_dw_epoch+=dl_dw\n",
        "      dl_db_epoch+=dl_db\n",
        "    dl_dw_epoch = dl_dw_epoch/n\n",
        "    dl_db_epoch = dl_db_epoch/n\n",
        "    w -= (eta*dl_dw_epoch)\n",
        "    b -= (eta*dl_db_epoch)\n",
        "    \n",
        "  theta = (w,b)\n",
        "  return theta \n",
        "\n",
        "\n",
        "def simplest_testing(theta, x):\n",
        "  # TODO: implement this method.\n",
        "  # Use the learned theta to classify given test data x.\n",
        "  # y stores the list of labels predicted by the model corresponding to each data in x.\n",
        "  (w,b) = theta\n",
        "  return w*x+b\n",
        "\n",
        "def simplest_loss(theta, x):\n",
        "  # TODO: implement this method for finding average loss\n",
        "  # Compute the average loss (use the loss function given in the question)\n",
        "  # for data points in x using theta.\n",
        "  # Return loss, a scalar value.\n",
        "  (w, b) = theta\n",
        "  lossvals = []\n",
        "  \n",
        "  for i in range(0, len(x)):\n",
        "    loss = (1/(1 + np.exp(-((w * i + b) - (3 * i + 2))**2))) - (1/2)\n",
        "    lossvals.append(loss)\n",
        "\n",
        "  sum_vals = sum(lossvals)\n",
        "  result_vals = sum_vals/len(lossvals)\n",
        "  return result_vals\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciBosF73d-mh",
        "outputId": "7cc78abe-698e-49ed-ba45-3ff09e419e8a"
      },
      "source": [
        "theta = simplest_training(30,10000,.02)\n",
        "eqn = simplest_testing(theta, np.arange(0,10))\n",
        "print(eqn)\n",
        "loss_vals = simplest_loss(theta, np.arange(0,10))\n",
        "print(loss_vals)\n",
        "\n",
        "theta = simplest_training(2,10000,.02)\n",
        "loss_vals = simplest_loss(theta, np.arange(0,10))\n",
        "print(loss_vals)\n",
        "\n",
        "\n",
        "theta = simplest_training(30,100,5)\n",
        "loss_vals = simplest_loss(theta, np.arange(0,10))\n",
        "print(loss_vals)\n",
        "\n",
        "\n",
        "theta = simplest_training(30,10000,.000002)\n",
        "loss_vals = simplest_loss(theta, np.arange(0,10))\n",
        "print(loss_vals)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2.05180272  4.91225119  7.77269966 10.63314812 13.49359659 16.35404506\n",
            " 19.21449353 22.074942   24.93539046 27.79583893]\n",
            "0.11334650988674352\n",
            "0.49507467561850776\n",
            "0.08264435497582397\n",
            "0.4982003413354832\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBZDJthCOAOw"
      },
      "source": [
        "### Analysis (10 Points)\n",
        "Answer the following questions (a - e):\n",
        "\n",
        "a) What is the average loss for test data for $x = (0, 1, ..., 9)$ using model\n",
        "with $n = 30, k = 10000, \\eta = 0.02$? (Only write the float value below)\n",
        "\n",
        "b) What is the average loss for test data for $x = (0, 1, ..., 9)$ using model\n",
        "with $n = 2, k = 10000, \\eta = 0.02$? (Only write the float value below)\n",
        "\n",
        "c) What is the average loss for test data for $x = (0, 1, ..., 9)$ using model\n",
        "with $n = 30, k = 100, \\eta = 5$? (Only write the float value below)\n",
        "\n",
        "d) What is the average loss for test data for $x = (0, 1, ..., 9)$ using model\n",
        "with $n = 30, k = 10000, \\eta = 0.000002$? (Only write the float value below)\n",
        "\n",
        "e) Are the average loss values you get in each cases above (b-d) differ from that (a)? Why? **(Explain in less than 50 words. You will be penalized if the answer exceeds the word limit.)**\n",
        "\n",
        "\\\\\n",
        "\n",
        "\n",
        "- !!! _YOUR RESPONSE HERE_ !!!\n",
        "\n",
        "**a)** 0.11334650988674352\n",
        "\n",
        "**b)** 0.49507467561850776\n",
        "\n",
        "**c)** 0.08264435497582397\n",
        "\n",
        "**d)** 0.4982003413354832\n",
        "\n",
        "**e)** Loss functions measure how good your prediction model does in terms of getting the expected outcome correct. with the goal of minimizing the result of the loss function. The avg loss values differ from 1) because the learning rate, num data points, and training repetitions are being adjusted.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KBU60MFa84o"
      },
      "source": [
        "## Problem 2 (35 Points)\n",
        "You will now create a network that is a little more complicated. It still contains just an input and an output layer, with no hidden layers. But it now has a nonlinearity along with a cross-entropy loss, so that we can use it for classification.\n",
        "<br>\n",
        "<br>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/thumbnail?id=1UkNx6-HghYRsjrXbqB6jN04VUsA-_RKp&sz=w1000\" alt=\"net2\" width=\"400px\"/>\n",
        "<br>\n",
        "<i>Figure 2: Network for Problem 2</i>\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "The network has two inputs, $x_1$ and $x_2$.  These are connected with two weights to a single output unit.  If we let $z = w_1x_1 + w_2x_2 + b$, the output unit will have an activation of $a = \\sigma(z)$, where $\\sigma(z)$ represents the sigmoid function:\n",
        "\n",
        "$$\\sigma(z) = \\frac{1}{1+e^{-z}}$$\n",
        "\n",
        "We can interpret the output as giving the probability that the input belongs to class 1. If the probability is low, then the input probably belongs to class 0. Hint: the derivative of the sigmoid is given by:\n",
        "\n",
        "$$\\frac{d\\sigma}{dz} = \\sigma(z)(1-\\sigma(z))$$\n",
        "\n",
        "In training the network, you will use the cross-entropy loss. In this case, the cross entropy loss will be:\n",
        "\n",
        "$$L_{CE}(x,y) = -(y\\log{a} + (1-y)\\log{(1-a)})$$\n",
        "\n",
        "If $y = 1$, this is just the negative log of what the network predicts for the probability that the input belongs to class 1.  If $y = 0$, it is the negative log of the probability that the input belongs to class 0.\n",
        "We provide you with a routine to generate training data. This has the form:\n",
        "\n",
        "```single_layer_training_data(trainset)```  \n",
        "    \n",
        "which returns $X$ and $y$.\n",
        "This provides two different training sets.  When the input, trainset, is 1, the function produces a simple, linearly separable training set.  Half the points are near $(0,0)$ and half are near $(10,10)$.  $X$ is a matrix in which each row contains one of these points, so it is $n \\times 2$, where $n$ is the number of points.  $y$ is a vector of class labels, which have the value 1 for the points near $(0,0)$ and 0 for the points near $(10,10)$.\n",
        "\n",
        "When trainset is 2, we generate a different training set that is not linearly separable, but that corresponds to the Xor problem.  Points from class 1 are either near $(0,0)$ or $(10,10)$, while points in class 0 are near either $(10,0)$ or $(0,10)$.\n",
        "\n",
        "You will need to implement two routines.  \n",
        "The first is: \n",
        "\n",
        "```single_layer_training(k, eta, trainset)```  \n",
        "  \n",
        "As before, $k$ will indicate the number of iterations of gradient descent and eta gives the learning rate.  trainset indicates which training set to use, 1 or 2.  You will train the network using the same gradient descent approach as in the previous problem.  As before, we suggest that you initialize weights using random values chosen from a Gaussian distribution with zero mean, and that you initialize bias at 0.  \n",
        "\n",
        "You will also implement a test routine: \n",
        "\n",
        "```single_layer_testing(theta, X)```  \n",
        "  \n",
        "This takes in the network parameters and a matrix, $X$, of the form returned by single\\_layer\\_training\\_data.  It returns a vector of the output values the network computes.\n",
        "\n",
        "__Remember__: The `trainset` argument is the integer to be used to generate data with `single_layer_training_data(trainset)`, it is NOT the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sLnCceZtHFv"
      },
      "source": [
        "###Problem 2\n",
        "###Provided function to create training data\n",
        "def single_layer_training_data(trainset):\n",
        "    n = 10\n",
        "    if trainset == 1:\n",
        "    # Linearly separable\n",
        "        X = np.concatenate((np.random.normal((0,0),1,(n,2)), np.random.normal((10,10),1,(n,2))),axis=0)\n",
        "        y = np.concatenate((np.ones(n), np.zeros(n)),axis=0)\n",
        "\n",
        "    elif trainset == 2:\n",
        "        # Not Linearly Separable\n",
        "        X = np.concatenate((np.random.normal((0,0),1,(n,2)), np.random.normal((10,10),1,(n,2)), np.random.normal((10,0),1,(n,2)), np.random.normal((0,10),1,(n,2))),axis=0)\n",
        "        y = np.concatenate((np.ones(2*n), np.zeros(2*n)), axis=0)\n",
        "\n",
        "    else:\n",
        "        print (\"function single_layer_training_data undefined for input\", trainset)\n",
        "        sys.exit()\n",
        "\n",
        "    return (X,y)\n",
        "\n",
        "def find_devs(x1, x2, y, w1, w2, b):\n",
        "  z = (w1*x1) + (w2*x2) + b\n",
        "  sigma = 1/(1+np.exp(-z))\n",
        "\n",
        "  dL_dsigma = -(y/sigma) + (1-y)/(1-sigma)\n",
        "\n",
        "  dsigma_ds = 1\n",
        "\n",
        "  ds_dz = (sigma)*(1-sigma)\n",
        "\n",
        "  dz_dw1 = x1\n",
        "  dz_dw2 = x2\n",
        "  dz_db = 1\n",
        "\n",
        "  dL_dw1 = dL_dsigma * dsigma_ds * ds_dz * dz_dw1\n",
        "  dL_dw2 = dL_dsigma  * dsigma_ds * ds_dz * dz_dw2\n",
        "  dL_db = dL_dsigma * dsigma_ds * ds_dz * dz_db\n",
        "\n",
        "  return (dL_dw1, dL_dw2, dL_db)\n",
        "\n",
        "def single_layer_training(k, eta, trainset):\n",
        "  #TODO: Your Code Here\n",
        "  w1 = np.random.normal(0,1)\n",
        "  w2 = np.random.normal(0,1)\n",
        "  b = 0\n",
        "\n",
        "  training_data = single_layer_training_data(trainset) \n",
        "  (X, y) = training_data\n",
        "  matrix = []\n",
        "\n",
        "  for i in range(len(X)):\n",
        "    matVals = [X[i][0], X[i][0], y[i]]\n",
        "    matrix.append(matVals)\n",
        "\n",
        "  np.random.shuffle(X)\n",
        "\n",
        "  for i in range(k):\n",
        "    t_w1 = 0\n",
        "    t_w2 = 0\n",
        "    t_b = 0\n",
        "    n = len(X)\n",
        "    for j in range(len(matrix)):\n",
        "      loss = find_devs(matrix[j][0], matrix[j][1], matrix[j][2], w1, w2, b)\n",
        "      (dw1, dw2, db) = loss\n",
        "      t_w1 += dw1\n",
        "      t_w2 += dw2\n",
        "      t_b += db\n",
        "    w1 -= (eta * (t_w1 / n))\n",
        "    w2 -= (eta * (t_w2 / n))\n",
        "    b -= (eta * (t_b / n))\n",
        "  return ((w1, w2), b)\n",
        "\n",
        "def single_layer_testing(theta, X):\n",
        "  #TODO: Your Code Here\n",
        "  ((w1, w2), b) = theta\n",
        "  sigmoidZs = []\n",
        "  for i in range(len(X)):\n",
        "    (x1, x2) = X[i] \n",
        "    z = (w1 * x1) + (w2 * x2) + b\n",
        "    sigma = 1/(1 + np.exp(-z))\n",
        "    sigmoidZs.append(sigma)\n",
        "    \n",
        "  print(sigmoidZs)\n",
        "  return sigmoidZs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "PGXjcb1HsTqI",
        "outputId": "eaad4c68-ac08-4ac7-85a2-f0e84bb6a970"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "((w1,w2),b) = single_layer_training(10000,.01,1)\n",
        "print(((w1,w2),b))\n",
        "(X,y) = single_layer_training_data(1)\n",
        "plt.scatter(X[:,0],X[:,1])\n",
        "single_layer_testing(((w1,w2),b),X)\n",
        "x = X[:,0]\n",
        "y = -(w1/w2)*x-(b/w2)\n",
        "plt.plot(x,y)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "((-1.1687472261840932, -0.016297270400337176), 3.7486868006711265)\n",
            "[0.9483912570427515, 0.9978731277873918, 0.9900208533685163, 0.859411394423279, 0.9900607338589897, 0.9801306752373988, 0.9759670177497659, 0.9661959326403218, 0.9832626733742033, 0.9973260279579421, 0.0005399304818221872, 7.590772263339853e-05, 0.0003582424113748611, 0.000478722638093387, 0.0003410713409269287, 0.00015013377372490372, 0.00017852490713513093, 0.0002985414221430242, 0.0011615578522583062, 0.00015422786709420937]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "yIPjk0iDHmc1",
        "outputId": "851ad0a4-d7fc-4d1d-89c2-7108cb391734"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "((w1,w2),b) = single_layer_training(10000,.0001,2)\n",
        "print(((w1,w2),b))\n",
        "(X,y) = single_layer_training_data(2)\n",
        "plt.scatter(X[:,0],X[:,1])\n",
        "single_layer_testing(((w1,w2),b),X)\n",
        "x = X[:,0]\n",
        "y = -(w1/w2)*x-(b/w2)\n",
        "plt.plot(x,y)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "((0.5795080770669806, -0.5914627936924958), 0.018212082802726383)\n",
            "[0.28284808015927726, 0.5388166682364285, 0.962424742191488, 0.7463448253442527, 0.6891054805556073, 0.6574825586811008, 0.6514365234357786, 0.34643011045474, 0.5877927361473143, 0.5933741485413023, 0.5879009293925279, 0.8030445647342483, 0.43792557344135075, 0.26391792522595525, 0.6995134263632634, 0.3716424248944848, 0.8364534000045191, 0.5369476182894489, 0.45439153181508346, 0.532093094000348, 0.9785297308894978, 0.9918140270756267, 0.9980706877343545, 0.9922413952758988, 0.9975617008780221, 0.9954496960989051, 0.9966783004400137, 0.9875163094776116, 0.9977528140393166, 0.9985419196991638, 0.0010148631795053441, 0.0006566422663819137, 0.0031099346276409227, 0.004772842985158183, 0.0007458358137676979, 0.010574275431838956, 0.008624645774088186, 0.0030030868775248677, 0.0046500946008242675, 0.004036021950989408]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKezMZl3VVrR",
        "outputId": "d02f9830-ae91-40f9-e118-da7b4687e0db"
      },
      "source": [
        "(X,y) = single_layer_training_data(1)\n",
        "(X1,y1) = single_layer_training_data(2)\n",
        "print(X)\n",
        "print(X1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.34704629 -0.09519928]\n",
            " [-0.22772779  0.43243668]\n",
            " [ 0.46693969 -1.3402814 ]\n",
            " [ 1.42285966  0.99379727]\n",
            " [-0.61651086  0.20493776]\n",
            " [-0.47954552  1.51238621]\n",
            " [-1.34683907 -2.78073771]\n",
            " [-0.0894738   0.55814156]\n",
            " [ 0.75572522  0.47821507]\n",
            " [-0.05390584 -1.24190139]\n",
            " [ 8.34150469  9.64662831]\n",
            " [11.6565077  11.51191283]\n",
            " [ 9.09319571  9.22278267]\n",
            " [ 9.26077191 10.9879891 ]\n",
            " [ 9.97352728  8.73733065]\n",
            " [ 8.43883486  9.06064615]\n",
            " [10.33545292  8.51024839]\n",
            " [ 8.93308925 11.03586292]\n",
            " [10.63131327  8.10563329]\n",
            " [10.03657141  9.68749781]]\n",
            "[[ 0.81424771  0.38240436]\n",
            " [ 1.64666591  0.22755337]\n",
            " [-1.3998746  -2.16131299]\n",
            " [ 0.28075023  0.40276401]\n",
            " [-0.71315197  0.81444016]\n",
            " [ 1.52375071 -0.59070442]\n",
            " [ 1.93479887 -0.92729301]\n",
            " [ 1.49433078 -0.82553065]\n",
            " [-0.70024184  0.3731913 ]\n",
            " [ 0.06268525 -1.02587916]\n",
            " [10.96301578  8.50771079]\n",
            " [11.09990115  9.71134036]\n",
            " [10.68145931 10.0319686 ]\n",
            " [10.9993242   7.57649067]\n",
            " [10.22760985 10.67641414]\n",
            " [10.04576622 10.3169048 ]\n",
            " [10.70293513 10.72661905]\n",
            " [12.08885756  9.29285197]\n",
            " [ 9.54298449 10.36950545]\n",
            " [10.97481869  9.63148649]\n",
            " [ 8.14395338  0.05021337]\n",
            " [ 9.58882218  0.04416541]\n",
            " [ 7.73677505 -0.24681177]\n",
            " [ 8.85238166 -0.29397299]\n",
            " [ 9.43983232  1.74827352]\n",
            " [10.30475977 -1.34689448]\n",
            " [ 9.45153883  0.45792694]\n",
            " [ 9.95621342 -0.68038383]\n",
            " [11.91826072  1.08073328]\n",
            " [11.34614637  0.23704865]\n",
            " [ 0.0204918  11.21258917]\n",
            " [-0.46221828 11.28413427]\n",
            " [-0.15320917 10.99577936]\n",
            " [ 0.10058525  9.6494235 ]\n",
            " [ 0.7761159  11.05589884]\n",
            " [-2.2603262   8.5428978 ]\n",
            " [ 1.46977145  8.96574964]\n",
            " [ 0.11494231  8.83620201]\n",
            " [ 0.98785397  9.68265042]\n",
            " [-0.68060621 10.38756993]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ty4jZ23sPvVA"
      },
      "source": [
        "### Analysis (10 Points)\n",
        "Do not use the data you used to train the network, call `single_layer_training_data` again to get fresh data for testing. Answer the following questions in this cell:\n",
        "\n",
        "a) What is the value of theta when k = 10000, $\\eta$=0.01, for trainset = 1? (((-1.1687472261840932, -0.016297270400337176), 3.7486868006711265)\n",
        "((w1,w2),b)\n",
        "\n",
        "b) Plot a figure for datapoints and the linearly separator the algorithm calculated under the condition in a).\n",
        "\n",
        "Done Above\n",
        "\n",
        "c) What is the value of theta when k = 10000, $\\eta$=0.0001, for trainset = 2? \n",
        "((0.5795080770669806, -0.5914627936924958), 0.018212082802726383)\n",
        "((w1,w2),b)\n",
        "\n",
        "d) Plot a figure for datapoints and the linearly separator the algorithm calculated under the condition in c).\n",
        "\n",
        "\n",
        "Done Above\n",
        "\n",
        "e) When trainset = 1, your network should assign a high probability of belonging to class 1 for points near $(0,0)$, and a low probability for points near $(10,10)$.  When trainset = 2, the data is not linearly separable, so you may find that your network has problems being able to separate. Use sample network outputs to describe what happens for the two trainsets. **(Explain in less than 50 words. You will be penalized if the answer exceeds the word limit.)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBqMD3voqug8"
      },
      "source": [
        "## Problem 3 (40 Points)\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/thumbnail?id=1lZQ1CnQUDD-kiyL-FtDu0UTO1dmEy-Oq&sz=w1000\" alt=\"net3\" width=\"400px\"/>\n",
        "<br>\n",
        "<i>Figure 3: Network for Problem 3</i>\n",
        "</center>\n",
        "<br>\n",
        "<br>\n",
        "Now you will implement a multi-layer network that has a hidden layer.  To start with a relatively simple case, we will do this without any non-linearities. The network has two input units, $x_1$ and $x_2$.  These are connected to a single hidden unit.  We'll call the activation of this hidden unit $h$, so $h = w_{11}x_1 + w_{12}x_2 + b_{11}$.  This hidden unit is connected to two output units.  We'll call their activation $z_1$ and $z_2$, so we have:\n",
        "\n",
        "$$z_1 = w_{21}h + b_{21}~~~~~~~~~~~z_2 = w_{22}h + b_{22}$$\n",
        "\n",
        "To train this network, we use a loss function that says that we want the output to be close to the input.  So the loss function is:  \n",
        "  \n",
        "$$L(x_1, x_2) = (z_1 - x_1)^2 + (z_2 - x_2)^2$$\n",
        "    \n",
        "That is, the input is also acting as the label.  This kind of network is called an **auto-encoder**.  You may be wondering what the point of this is.  Because the hidden layer is smaller than the input and output layers, the network is forced to learn low-dimensional representation of the data.  In this case, the network learns to map the input points onto a line in the hidden layer, and then compute the 2D coordinates of the points on this line for the output layer.  This process is called Principal Component Analysis (PCA).\n",
        "\n",
        "We will provide a routine to generate training data:\n",
        "\n",
        "```pca_training_data(n, sigma)```  \n",
        "  \n",
        "The input parameter $n$ indicates the number of points in the training set.  As in the last problem, $X$ contains a $n \\times 2$ matrix in which each row contains the coordinates of a 2D point.  These points are generated to lie along the line $y = x + 1$.  Then Gaussian noise is added to the points, with zero mean and a standard deviation of sigma.\n",
        "\n",
        "Once again, you will implement training and testing routines.  \n",
        "  \n",
        "`pca_training(k, eta, n, sigma)`  \n",
        "  \n",
        "The input $k$ gives the number of iterations of gradient descent to use, while $eta$ gives the learning rate.  The input value $n$ indicates the number of points in the training set, while $sigma$ indicates the amount of noise added to these points.  Use these as parameters to pca\\_training\\_data.  The routine returns theta, a representation of all the weights and biases in the network.\n",
        "Also implement a test routine: \n",
        "\n",
        "```pca_test(theta, X)```  \n",
        "  \n",
        "$X$ will contain test data in the form returned by pca\\_training\\_data.  $Z$ provides the results the network produces given this input; $Z$ has the same format as $X$.  \n",
        "\n",
        "To test this, try training the network with $n = 10$ and $sigma = .1$.  Then test, using the input: `pca_test(theta, [[1,2], [4,5], [10, 3]])`.  \n",
        "  \n",
        "When I run my  code with this test I get: `[[0.9418, 2.0653], [3.9543, 5.0511], [6.1780, 7.2551]]`.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OI3FF-JptKqI"
      },
      "source": [
        "###Problem 3\n",
        "###Provided function to create training data\n",
        "def pca_training_data(n, sigma):\n",
        "    m = 1\n",
        "    b = 1\n",
        "    x1 = np.random.uniform(0,10,n)\n",
        "    x2 = m*x1+b\n",
        "    X = np.c_[x1, x2]\n",
        "    X += np.random.normal(0, sigma, X.shape)\n",
        "    return X\n",
        "\n",
        "def find_devsLarge(x1,x2,theta):\n",
        "  ws,bs = theta\n",
        "  (w11, w12, w21, w22) = ws\n",
        "  (b11,b21,b22) = bs\n",
        "\n",
        "  h = (w11 * x1) + (w12 * x2) + b11\n",
        "\n",
        "  z1 = (w21 * h) + b21\n",
        "  z2 = (w22 * h) + b22\n",
        "\n",
        "  dL_dh = 2.0 * w21 * (z1 - x1) + 2.0 * w22 * (z2 - x2)\n",
        "\n",
        "  dh_db11 = 1\n",
        "  dL_b11 = dL_dh * dh_db11\n",
        "\n",
        "  dL_dz1 = 2.0 * (z1 - x1)\n",
        "  dz1_db21 = 1.0\n",
        "  dz2_db22 = 1.0\n",
        "  dL_db21 = dL_dz1 * dz1_db21\n",
        "  dL_dz2 = 2.0 * (z2 - x2)\n",
        "  \n",
        "  dL_db22 = dL_dz2 * dz2_db22\n",
        "\n",
        "  bs = (dL_b11, dL_db21, dL_db22)\n",
        "\n",
        "  dL_dz1 = 2.0 * (z1 - x1)\n",
        "  dz1_dw21 = h\n",
        "  dL_dw21 = dL_dz1 * dz1_dw21\n",
        "\n",
        "  dL_dz2 = 2.0 * (z2 - x2)\n",
        "  dz2_dw22 = h\n",
        "  dL_dw22 = dL_dz2 * dz2_dw22\n",
        "\n",
        "  dh_dw11 = x1\n",
        "  dL_dw11 = dL_dh * dh_dw11\n",
        "\n",
        "  dh_dw12 = x2\n",
        "  dL_dw12 = dL_dh * dh_dw12\n",
        "\n",
        "  ws = (dL_dw11, dL_dw12, dL_dw21, dL_dw22)\n",
        "  return (ws, bs) \n",
        "  \n",
        "\n",
        "def pca_training(k, eta, n, sigma):\n",
        "    #TODO: Your Code Here\n",
        "    pca_TD = pca_training_data(n, sigma)\n",
        "    b11, b21, b22 = 0.0, 0.0, 0.0\n",
        "    w11, w12, w21, w22 = np.random.normal(0,1), np.random.normal(0,1), np.random.normal(0,1), np.random.normal(0,1)\n",
        "    n = len(pca_TD)\n",
        "\n",
        "    for i in range(k):\n",
        "      tot_b11, tot_b21, tot_b22 = 0, 0, 0\n",
        "      tot_w11, tot_w12, tot_w21, tot_w22 = 0, 0, 0, 0\n",
        "    \n",
        "      for j in range(len(pca_TD)):\n",
        "\n",
        "        x1, x2 = pca_TD[j]\n",
        "        ws = (w11, w12, w21, w22)\n",
        "        bs = (b11, b21, b22)\n",
        "        theta = (ws, bs)\n",
        "\n",
        "        dW, dB = find_devsLarge(x1, x2, theta)\n",
        "\n",
        "        (dw11, dw12, dw21, dw22) = dW\n",
        "        (db11, db21, db22) = dB\n",
        "\n",
        "        tot_w11 += dw11\n",
        "        tot_w12 += dw12\n",
        "        tot_w21 += dw21\n",
        "        tot_w22 += dw22\n",
        "        tot_b11 += db11\n",
        "        tot_b21 += db21\n",
        "        tot_b22 += db22\n",
        "\n",
        "      # print((w11, w12, w21, w22))\n",
        "      w11 -= (eta * (tot_w11/n))\n",
        "      w12 -= (eta * (tot_w12/n))\n",
        "      w21 -= (eta * (tot_w21/n))\n",
        "      w22 -= (eta * (tot_w22/n))\n",
        "      b11 -= (eta * (tot_b11/n))\n",
        "      b21 -= (eta * (tot_b21/n))\n",
        "      b22 -= (eta * (tot_b22/n))\n",
        "      # print((w11, w12, w21, w22))\n",
        "    ws = (w11, w12, w21, w22)\n",
        "    bs = (b11, b21, b22)\n",
        "    theta = (ws, bs)\n",
        "    return theta\n",
        "\n",
        "def pca_test(theta, X):\n",
        "    #TODO: Your Code Here\n",
        "    ws,bs = theta\n",
        "    w11, w12, w21, w22 = ws\n",
        "    b11, b21, b22 = bs\n",
        "    resultVals = []\n",
        "    for i in range(len(X)):\n",
        "      x1, x2 = X[i]\n",
        "      h = (w11 * x1) + (w12 * x2) + b11\n",
        "      z1 = (w21 * h) + b21\n",
        "      z2 = (w22 * h) + b22\n",
        "      resultVals.append([z1, z2])\n",
        "    return resultVals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17WXODJLefC0",
        "outputId": "886872e2-595c-401a-899e-b9fbe76ba829"
      },
      "source": [
        "theta = pca_training(100,.2,10,.1)\n",
        "pca_test(theta, [[1,2], [4,5], [10, 3]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: RuntimeWarning: overflow encountered in double_scalars\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: RuntimeWarning: overflow encountered in double_scalars\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in double_scalars\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[nan, nan], [nan, nan], [nan, nan]]"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwXLKsE9QQjt"
      },
      "source": [
        "### Analysis (10 Points)\n",
        "It may take a little work to find good values for $k$ and $\\eta$.  Add a description of your experimental results inside this cell.  \n",
        "\n",
        "a) Can you explain why the network produces the point $(6.1780, 7.2551)$ with an input of $(10, 3)$? **(Explain in less than 30 words. You will be penalized if the answer exceeds the word limit.)**\n",
        "\n",
        "b) Do another test with $sigma = 0$ instead of $sigma = .1$.  Run your network with the same test data.  How have the results changed?  Can you explain this change? **(Explain in less than 30 words. You will be penalized if the answer exceeds the word limit.)**\n",
        "\n",
        "\\\\\n",
        "\n",
        "- !!! _YOUR RESPONSE HERE_ !!!\n",
        "\n",
        "**a)** The input (10,3) won't fit the line y = x + 1, so the point the network produces will not fit also.\n",
        "\n",
        "**b)** There's no noise with sigma = 0 instead of sigma = .1 so the network is able to fit the test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Wep92fjrcgK"
      },
      "source": [
        "## Problem 4 (optional challenge problem, for extra credit, 20 points):\n",
        "Ok, now you are ready to create a complete, fully connected neural net with a hidden layer and non-linearities. You will use this network to solve the XOR problem, using the same training data as in Problem 2. Your network architecture should have the following components:\n",
        "- Two input units, with activations $x_1$ and $x_2$.  These are just the coordinates of 2D points.\n",
        "- A variable number of hidden units, H.  Write your code so that you can select the number of hidden units as a hyperparameter.  Let's call the activation of the $i$'th hidden unit, $a^1_i$.  Let's call the weights of these units $w^1_{ij}$.  This is the weight from input unit $j$ to hidden unit $i$.  \n",
        "- Use a RELU non-linearity for the hidden units.  So to determine the activation of a hidden unit we have: $z^1_i = w^1_{i1}x_1 + w^1_{i2}x_2 + b^1_i$, and $a^1_i = max(0, z^1_i)$.\n",
        "- There is then a single output unit.  Call its activation $a^2$.  We compute this as: $z^2 = \\left( \\sum_{i=1}^H w^2_{1i}a^1_i \\right) + b^2$, and $a^2 = \\sigma(z^2)$, where $\\sigma$ is the sigmoid nonlinearity.  This last part is just the same as in Problem 2.  And, like Problem 2, you can train your network using the cross-entropy loss.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/thumbnail?id=1qfLXZJsDFIwTfIdHxEP6HnJBmUrPGZsT&sz=w1000\" alt=\"net3\" width=\"400px\"/>\n",
        "<br>\n",
        "<i>Figure 4: Network for Problem 4</i>\n",
        "</center>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "Implement test and training functions with the templates:\n",
        "\n",
        "`nn_training(k, eta, trainset, H)`\n",
        "\n",
        "`nn_testing(theta, X)`\n",
        "\n",
        "The parameters to the training routine are similar to those in Problem 2, with $H$ indicating the number of hidden units.  The testing routine has the same form as in Problem 2.\n",
        "\n",
        "__Remember__: The `trainset` argument is the integer to be used to generate data with `single_layer_training_data(trainset)`, it is not the actual training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3U68nvJtM1_"
      },
      "source": [
        "###Problem 4: Challenge Problem\n",
        "def nn_training(k, eta, trainset, H):\n",
        "    #TODO: Your Code Here\n",
        "    return theta\n",
        "\n",
        "def nn_testing(theta, X):\n",
        "    #TODO: Your Code Here\n",
        "    return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJcDH94VQ3iV"
      },
      "source": [
        "### Analysis (5 Points)\n",
        "Run experiments to demonstrate that your network can solve the XOR problem. How do you find the results vary as you vary the number of hidden units?  Show and discuss the results of your experiments inside/below this cell.\n",
        "\n",
        "- !!! _YOUR RESPONSE HERE_ !!!"
      ]
    }
  ]
}